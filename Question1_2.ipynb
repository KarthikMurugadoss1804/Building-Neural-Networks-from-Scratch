{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training features  (60000, 784)\n",
      "Shape of test features  (10000, 784)\n",
      "Shape of training labels  (10, 60000)\n",
      "Shape of testing labels  (10, 10000)\n",
      "Shape of reshaped training features  (784, 60000)\n",
      "Shape of reshaped test features  (784, 10000)\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "# load the training and test data    \n",
    "(tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
    "\n",
    "# reshape the feature data\n",
    "tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
    "te_x = te_x.reshape(te_x.shape[0], 784)\n",
    "\n",
    "# noramlise feature data\n",
    "tr_x = tr_x / 255.0\n",
    "te_x = te_x / 255.0\n",
    "\n",
    "print( \"Shape of training features \", tr_x.shape)\n",
    "print( \"Shape of test features \", te_x.shape)\n",
    "\n",
    "\n",
    "# one hot encode the training labels and get the transpose\n",
    "tr_y = np_utils.to_categorical(tr_y,10)\n",
    "tr_y = tr_y.T\n",
    "print (\"Shape of training labels \", tr_y.shape)\n",
    "\n",
    "# one hot encode the test labels and get the transpose\n",
    "te_y = np_utils.to_categorical(te_y,10)\n",
    "te_y = te_y.T\n",
    "print (\"Shape of testing labels \", te_y.shape)\n",
    "\n",
    "#Reshaping Training data to n x m matrix \n",
    "tr_x_reshaped = tr_x.T \n",
    "tr_y = tf.constant(tr_y)\n",
    "tr_x_reshaped = tf.constant(tr_x_reshaped)\n",
    "#Reshaping test data to n x m matrix \n",
    "te_x_reshaped = te_x.T\n",
    "te_y = tf.constant(te_y)\n",
    "te_x_reshaped = tf.constant(te_x_reshaped)\n",
    "print( \"Shape of reshaped training features \", tr_x_reshaped.shape)\n",
    "print( \"Shape of reshaped test features \", te_x_reshaped.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "W1 = tf.Variable(tf.random.normal([300,tr_x_reshaped.shape[0]], mean=0.0, stddev=0.05))\n",
    "b1 = tf.Variable(tf.zeros(shape = (300,1)))\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([10,300], mean=0.0, stddev=0.05))\n",
    "b2 = tf.Variable(tf.zeros(shape = (10,1)))\n",
    "\n",
    "def forward_pass(X,W1,W2,b1,b2):\n",
    "    #Adding Relu Layer\n",
    "    A1 = tf.add(tf.matmul(tf.cast(W1, tf.float32),tf.cast(X, tf.float32)), b1)\n",
    "    H1 = tf.nn.relu(A1)\n",
    "    A2 = tf.add(tf.matmul(tf.cast(W2, tf.float32),tf.cast(H1, tf.float32)), b2)\n",
    "    H2 = softmax(A2)\n",
    "    H2 = tf.clip_by_value(H2, 1e-10, 1.0)\n",
    "    return H2\n",
    "\n",
    "def cross_entropy(H,y):\n",
    "    loss  = -tf.reduce_sum(tf.math.multiply(y, tf.math.log(H)),0)\n",
    "    total_loss = tf.reduce_mean(loss)\n",
    "    return total_loss\n",
    "    \n",
    "    \n",
    "def calculate_accuracy(y_pred, y):\n",
    "    predictions = tf.math.argmax(y_pred)\n",
    "    actual_y = tf.math.argmax(y)\n",
    "    predictions_correct = tf.cast(tf.equal(predictions, actual_y), tf.float32)\n",
    "    accuracy = tf.reduce_mean(predictions_correct)\n",
    "    return accuracy\n",
    "\n",
    "def softmax(A):\n",
    "    t = tf.math.exp(A)\n",
    "    sum_of_t = tf.reduce_sum(t, 0)\n",
    "    sum_of_t = tf.broadcast_to(sum_of_t, t.shape)\n",
    "    soft_max_output = tf.math.divide(t, sum_of_t)\n",
    "    return soft_max_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 : Loss =  2.3610342   Acc:  0.0565\n"
     ]
    }
   ],
   "source": [
    "num_Iterations = 500\n",
    "adam_optimizer = tf.keras.optimizers.Adam()\n",
    "trainingLoss= []\n",
    "validationLoss= []\n",
    "trainingAccuracies = []\n",
    "validationAccuracies = []\n",
    "\n",
    "for i in range(num_Iterations):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = forward_pass(tr_x_reshaped, W1,W2, b1,b2)\n",
    "        currentLoss = cross_entropy(y_pred,tr_y)\n",
    "        gradients = tape.gradient(currentLoss, [W1,W2, b1,b2])\n",
    "    accuracy = calculate_accuracy(y_pred, tr_y)\n",
    "    trainingLoss.append(currentLoss)\n",
    "    trainingAccuracies.append(accuracy)\n",
    "    # Calculating Test Loss ana accuracies\n",
    "    y_pred_test = forward_pass(te_x_reshaped, W1,W2, b1,b2)\n",
    "    currentLoss_test = cross_entropy(y_pred_test,te_y)\n",
    "    accuracy_test = calculate_accuracy(y_pred_test, te_y)\n",
    "    validationLoss.append(currentLoss_test)\n",
    "    validationAccuracies.append(accuracy_test)\n",
    "    if i%100 == 0:\n",
    "        print (\"Iteration \", i, \": Loss = \",currentLoss.numpy(), \"  Acc: \", accuracy.numpy())\n",
    "    adam_optimizer.apply_gradients(zip(gradients, [W1,W2, b1,b2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(validationLoss, label=\"Val Loss\")\n",
    "plt.plot(validationAccuracies, label=\"Val Acc\")\n",
    "plt.plot(trainingLoss, label=\"Train Loss\")\n",
    "plt.plot(trainingAccuracies, label=\"Train Acc\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random.normal([300,tr_x_reshaped.shape[0]], mean=0.0, stddev=0.05))\n",
    "b1 = tf.Variable(tf.zeros(shape = (300,1)))\n",
    "W2 = tf.Variable(tf.random.normal([100,300], mean=0.0, stddev=0.05))\n",
    "b2 = tf.Variable(tf.zeros(shape = (100,1)))\n",
    "W3 = tf.Variable(tf.random.normal([10,100], mean=0.0, stddev=0.05))\n",
    "b3 = tf.Variable(tf.zeros(shape = (10,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_B(X,W1,W2,W3,b1,b2,b3):\n",
    "    #Adding Relu Layer\n",
    "    A1 = tf.add(tf.matmul(tf.cast(W1, tf.float32),tf.cast(X, tf.float32)), b1)\n",
    "    H1 = tf.nn.relu(A1)\n",
    "    A2 = tf.add(tf.matmul(tf.cast(W2, tf.float32),tf.cast(H1, tf.float32)), b2)\n",
    "    H2 = tf.nn.relu(A2)\n",
    "    A3 = tf.add(tf.matmul(tf.cast(W3, tf.float32),tf.cast(H2, tf.float32)), b3)\n",
    "    H3 = softmax(A3)\n",
    "    H3 = tf.clip_by_value(H3, 1e-10, 1.0)\n",
    "    return H3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingLoss_B= []\n",
    "validationLoss_B= []\n",
    "trainingAccuracies_B = []\n",
    "validationAccuracies_B = []\n",
    "\n",
    "for i in range(num_Iterations):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = forward_pass_B(tr_x_reshaped, W1,W2,W3,b1,b2,b3)\n",
    "        currentLoss = cross_entropy(y_pred,tr_y)\n",
    "        gradients = tape.gradient(currentLoss, [W1,W2,W3, b1,b2,b3])\n",
    "    accuracy = calculate_accuracy(y_pred, tr_y)\n",
    "    trainingLoss_B.append(currentLoss)\n",
    "    trainingAccuracies_B.append(accuracy)\n",
    "    # Calculating Test Loss ana accuracies\n",
    "    y_pred_test = forward_pass_B(te_x_reshaped, W1,W2,W3, b1,b2,b3)\n",
    "    currentLoss_test = cross_entropy(y_pred_test,te_y)\n",
    "    accuracy_test = calculate_accuracy(y_pred_test, te_y)\n",
    "    validationLoss_B.append(currentLoss_test)\n",
    "    validationAccuracies_B.append(accuracy_test)\n",
    "    if i%100 == 0:\n",
    "        print (\"Iteration \", i, \": Loss = \",currentLoss.numpy(), \"  Acc: \", accuracy.numpy())\n",
    "    adam_optimizer.apply_gradients(zip(gradients, [W1,W2,W3,b1,b2,b3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(validationLoss_B, label=\"Test Loss\")\n",
    "plt.plot(validationAccuracies_B, label=\"Test Acc\")\n",
    "plt.plot(trainingLoss_B, label=\"Train Loss\")\n",
    "plt.plot(trainingAccuracies_B, label=\"Train Acc\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
