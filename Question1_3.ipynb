{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training features  (60000, 784)\n",
      "Shape of test features  (10000, 784)\n",
      "Shape of training labels  (10, 60000)\n",
      "Shape of testing labels  (10, 10000)\n",
      "Shape of training features  (784, 60000)\n",
      "Shape of test features  (784, 10000)\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "# load the training and test data    \n",
    "(tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
    "\n",
    "# reshape the feature data\n",
    "tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
    "te_x = te_x.reshape(te_x.shape[0], 784)\n",
    "\n",
    "# noramlise feature data\n",
    "tr_x = tr_x / 255.0\n",
    "te_x = te_x / 255.0\n",
    "\n",
    "print( \"Shape of training features \", tr_x.shape)\n",
    "print( \"Shape of test features \", te_x.shape)\n",
    "\n",
    "\n",
    "# one hot encode the training labels and get the transpose\n",
    "tr_y = np_utils.to_categorical(tr_y,10)\n",
    "tr_y = tr_y.T\n",
    "print (\"Shape of training labels \", tr_y.shape)\n",
    "\n",
    "# one hot encode the test labels and get the transpose\n",
    "te_y = np_utils.to_categorical(te_y,10)\n",
    "te_y = te_y.T\n",
    "print (\"Shape of testing labels \", te_y.shape)\n",
    "\n",
    "#Reshaping Training data to n x m matrix \n",
    "tr_x_reshaped = tr_x.T \n",
    "tr_y = tf.constant(tr_y)\n",
    "tr_x_reshaped = tf.constant(tr_x_reshaped)\n",
    "#Reshaping test data to n x m matrix \n",
    "te_x_reshaped = te_x.T\n",
    "te_y = tf.constant(te_y)\n",
    "te_x_reshaped = tf.constant(te_x_reshaped)\n",
    "print( \"Shape of training features \", tr_x_reshaped.shape)\n",
    "print( \"Shape of test features \", te_x_reshaped.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "W1 = tf.Variable(tf.random.normal([300,tr_x_reshaped.shape[0]], mean=0.0, stddev=0.05))\n",
    "b1 = tf.Variable(tf.zeros(shape = (300,1)))\n",
    "W2 = tf.Variable(tf.random.normal([100,300], mean=0.0, stddev=0.05))\n",
    "b2 = tf.Variable(tf.zeros(shape = (100,1)))\n",
    "W3 = tf.Variable(tf.random.normal([10,100], mean=0.0, stddev=0.05))\n",
    "b3 = tf.Variable(tf.zeros(shape = (10,1)))\n",
    "reg_rate = tf.constant(0.00001)\n",
    "\n",
    "def reg_value_L1(list_of_weights):\n",
    "    return reduce(lambda a,b: tf.reduce_sum(a)+tf.reduce_sum(b), list_of_weights)\n",
    "\n",
    "def reg_value_L2(list_of_weights):\n",
    "    return reduce(lambda a,b: tf.reduce_sum(tf.math.square(a))+tf.reduce_sum(tf.math.square(b)), list_of_weights)\n",
    "    \n",
    "    \n",
    "def forward_pass_B(X,W1,W2,W3,b1,b2,b3):\n",
    "    #Adding Relu Layer\n",
    "    A1 = tf.add(tf.matmul(tf.cast(W1, tf.float32),tf.cast(X, tf.float32)), b1)\n",
    "    H1 = tf.nn.relu(A1)\n",
    "    A2 = tf.add(tf.matmul(tf.cast(W2, tf.float32),tf.cast(H1, tf.float32)), b2)\n",
    "    H2 = tf.nn.relu(A2)\n",
    "    A3 = tf.add(tf.matmul(tf.cast(W3, tf.float32),tf.cast(H2, tf.float32)), b3)\n",
    "    H3 = softmax(A3)\n",
    "    H3 = tf.clip_by_value(H3, 1e-10, 1.0)\n",
    "    return H3\n",
    "\n",
    "def cross_entropy(H,y,W1,W2,W3):\n",
    "    global reg_rate\n",
    "    loss  = -tf.reduce_sum(tf.math.multiply(y, tf.math.log(H)),0)\n",
    "    reg_value = tf.multiply(reg_value_L1([W1,W2,W3]) , reg_rate)\n",
    "    total_loss = tf.add(tf.reduce_mean(loss),reg_value)\n",
    "    return total_loss\n",
    "def cross_entropy_L2(H,y,W1,W2,W3):\n",
    "    global reg_rate\n",
    "    loss  = -tf.reduce_sum(tf.math.multiply(y, tf.math.log(H)),0)\n",
    "    reg_value = tf.multiply(reg_value_L2([W1,W2,W3]), reg_rate)\n",
    "    total_loss = tf.add(tf.reduce_mean(loss), reg_value)\n",
    "    return total_loss\n",
    "    \n",
    "    \n",
    "def calculate_accuracy(y_pred, y):\n",
    "    predictions = tf.math.argmax(y_pred)\n",
    "    actual_y = tf.math.argmax(y)\n",
    "    predictions_correct = tf.cast(tf.equal(predictions, actual_y), tf.float32)\n",
    "    accuracy = tf.reduce_mean(predictions_correct)\n",
    "    return accuracy\n",
    "\n",
    "def softmax(A):\n",
    "    t = tf.math.exp(A)\n",
    "    sum_of_t = tf.reduce_sum(t, 0)\n",
    "    soft_max_output = tf.math.divide(t, sum_of_t)\n",
    "    return soft_max_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 : Loss =  2.3133523   Acc:  0.052033335\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_Iterations = 500\n",
    "adam_optimizer = tf.keras.optimizers.Adam()\n",
    "trainingLoss= []\n",
    "validationLoss= []\n",
    "trainingAccuracies = []\n",
    "validationAccuracies = []\n",
    "\n",
    "for i in range(num_Iterations):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = forward_pass_B(tr_x_reshaped, W1,W2,W3,b1,b2,b3)\n",
    "        currentLoss = cross_entropy(y_pred,tr_y,W1,W2,W3)\n",
    "        gradients = tape.gradient(currentLoss, [W1,W2,W3, b1,b2,b3])\n",
    "    accuracy = calculate_accuracy(y_pred, tr_y)\n",
    "    trainingLoss.append(currentLoss)\n",
    "    trainingAccuracies.append(accuracy)\n",
    "    # Calculating Test Loss ana accuracies\n",
    "    y_pred_test = forward_pass_B(te_x_reshaped, W1,W2,W3, b1,b2,b3)\n",
    "    currentLoss_test = cross_entropy(y_pred_test,te_y, W1,W2,W3)\n",
    "    accuracy_test = calculate_accuracy(y_pred_test, te_y)\n",
    "    validationLoss.append(currentLoss_test)\n",
    "    validationAccuracies.append(accuracy_test)\n",
    "    if i%100 == 0:\n",
    "        print (\"Iteration \", i, \": Loss = \",currentLoss.numpy(), \"  Acc: \", accuracy.numpy())\n",
    "    adam_optimizer.apply_gradients(zip(gradients, [W1,W2,W3,b1,b2,b3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(0, 500, 1)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Iterations')\n",
    "ax1.set_ylabel('Loss', color=color)\n",
    "ax1.plot(t,validationLoss, color=color)\n",
    "ax1.plot(t, trainingLoss, color='tab:green')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Accuracy', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(t, validationAccuracies, color=color)\n",
    "ax2.plot(t, trainingAccuracies, color='tab:orange' )\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random.normal([300,tr_x_reshaped.shape[0]], mean=0.0, stddev=0.05))\n",
    "b1 = tf.Variable(tf.zeros(shape = (300,1)))\n",
    "W2 = tf.Variable(tf.random.normal([100,300], mean=0.0, stddev=0.05))\n",
    "b2 = tf.Variable(tf.zeros(shape = (100,1)))\n",
    "W3 = tf.Variable(tf.random.normal([10,100], mean=0.0, stddev=0.05))\n",
    "b3 = tf.Variable(tf.zeros(shape = (10,1)))\n",
    "reg_rate = tf.constant(0.01)\n",
    "\n",
    "trainingLoss_B= []\n",
    "validationLoss_B= []\n",
    "trainingAccuracies_B = []\n",
    "validationAccuracies_B = []\n",
    "\n",
    "for i in range(num_Iterations):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = forward_pass_B(tr_x_reshaped, W1,W2,W3,b1,b2,b3)\n",
    "        currentLoss = cross_entropy_L2(y_pred,tr_y,W1,W2,W3)\n",
    "        gradients = tape.gradient(currentLoss, [W1,W2,W3, b1,b2,b3])\n",
    "    accuracy = calculate_accuracy(y_pred, tr_y)\n",
    "    trainingLoss_B.append(currentLoss)\n",
    "    trainingAccuracies_B.append(accuracy)\n",
    "    # Calculating Test Loss ana accuracies\n",
    "    y_pred_test = forward_pass_B(te_x_reshaped, W1,W2,W3, b1,b2,b3)\n",
    "    currentLoss_test = cross_entropy_L2(y_pred_test,te_y, W1,W2,W3)\n",
    "    accuracy_test = calculate_accuracy(y_pred_test, te_y)\n",
    "    validationLoss_B.append(currentLoss_test)\n",
    "    validationAccuracies_B.append(accuracy_test)\n",
    "    if i%100 == 0:\n",
    "        print (\"Iteration \", i, \": Loss = \",currentLoss.numpy(), \"  Acc: \", accuracy.numpy())\n",
    "    adam_optimizer.apply_gradients(zip(gradients, [W1,W2,W3,b1,b2,b3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "t = np.arange(0, 500, 1)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Iterations')\n",
    "ax1.set_ylabel('Loss', color=color)\n",
    "ax1.plot(t,validationLoss_B, color=color)\n",
    "ax1.plot(t, trainingLoss_B, color='tab:green')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Accuracy', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(t, validationAccuracies_B, color=color)\n",
    "ax2.plot(t, trainingAccuracies_B, color='tab:orange' )\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
